When do you think AGI will be created? Oh boy. I think it'll happen in our lifetime, but I don't think it's for the next like 10 or 20 years. This is one of the things that Robert and I disagree on. I think that really depends on how you define AGI. It really depends on how you define it. It really depends on the definition. If you say, you know, like high-level machine intelligence, meaning it can do anything that a human can do, at least as well as the best human, then probably at least a decade or two. It'll be created in a couple of years. Four to five years. Roughly 2035. This idea that inside some foundation model there is an autonomous agent with thoughts and beliefs and all of these things on its own, where if we were to put that information in a human body, it would act like a human or have a soul, whatever we consider human. No, I think that's all BS. What I think about AGI in terms of there's an intent behind actions, that there's something like consciousness, I think we're very far from that. It's not like chat GPT has sentience or anything like that. It's basically doing causal mass language modeling. So it's trying to predict the next token. And that's a very simple objective function. We don't even know how humans think or how humans work. So understanding what AGI is is really hard to quantify because we don't know what humans are. How are we intelligent and why are we intelligent? And if it just mimics what we believe to be human in the first place, maybe that's enough. Maybe it'll be like three years, five years, who knows? I think the harder question is like, how do you know when you achieve it even? I haven't reconciled what does true AGI mean? I think at best, I believe it's a spectrum. And I think some people will call some level of capabilities AGI before others. So I feel it's actually pretty close. If you use GPT-4 in particular, when it exhibits logic skills, so whenever you're able to just paste the things and it can kind of infer what you want, those are the moments that make me think like, wow, this is really getting close. I think AGI to some extent is already out there today. I think that the tools that we have today, even though they're not completely general, can be assembled already to solve arbitrarily complex problems. I already thought it was here. I thought GPT-3 was AGI. I think we're here. And so I think we're already here and it's just gonna be like a slow moving continuum. Let me put it this way. I'm not gonna give you a timeline, but I'm gonna give you something analogous to the timeline. I think it's somewhere between five and 10 research breakthroughs away. That's so much better than putting it in terms of years. No idea how many years that is. Who knows? I think we're still very far away from that being true. I have no idea about the date or anything like that. What I will tell you is that like, I think it's a world that none of us actually have ever been able to even dream of. Like, I don't even know whether this interview will be happening this way if AGI really existed. I think it's probably a lot further away than a lot of people think. Suddenly, I mean, you know, I talk to my family, I talk to friends back home and it feels like it's right around the corner because, you know, you use something like GPT-4 and it feels so real. It feels so human-like and, you know, it's very good at doing what it does, which is the language stuff. But as we're learning, trying to build out these brains for these characters in our video games, there's a lot that has to go into a brain beyond language. Oh yeah, I don't think we're anywhere close. I don't even think what we do should be called AI. It's definitely not intelligent. It's really just connecting images to data. And if you play with it for 30 seconds, you see that it's not very smart. Like, they'll really confidently say, like, yes, 50 million is greater than 100 million, right? It's like the kinds of mistakes that humans would never make and, you know, it's not necessary that the thought process of an AGI has to look like what a human's does, but it is concerning in terms of, like, whether or not they're actually reasoning or whether they are truly just, like, you know, matching patterns and predicting statistically. I don't think AGI is coming from any of the current stuff we're doing. My idea is probably we're already in the AGI world. It's just we don't feel it. Wow, so two very different responses. Yeah. Do you think it will have a positive or a negative impact on society if it ever happens? I think it's, like, there's not a set way in which it will go. I think it really does rely on our ability to, like, agree on core principles around both the development and deployment of the models to be able to, like, control how it's able to both benefit society but also stay within, like, the boundaries of what we want this type of technology to do. But there will be a lot of interesting moral and philosophical questions that will, like, come up as we get closer. I'm a big fan of AI as the human in the driver's seat and as a helper. Honestly, a lot of the ethics questions, I think, should come up when it gets to AGI. If we, like, picture a world where, you know, AGI is, like, super prevalent, like, ideally, again, it's a work that it's doing is based on human values and there's proper governance so that those models can actually, you know, help people live more fruitful and connected lives. AI already passes the Turing test. So I think there'll be a list of tasks where we'll keep, you know, asking ourselves, can AI do this as good or better than a human? And when we can no longer think of things that humans can still do better than the AI, you know, when we really have to kind of, like, rack our brains, like, huh, what do humans do better? I think that'll be kind of the moment, like, in the collective consciousness when it's crossed over. I'm not gonna put a timeline on it, but I'm just excited to see where things go.