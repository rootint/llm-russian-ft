My name is Corey. I'm the CEO at Spellrush and I'm here to talk to you today about designing characters with deep learning. So we're Spellrush. We're a YC company as well. We're building deep learning tools for art and artists. What exactly does this mean? Art is hard. So my co-founder is a professional artist but we're a fairly small team so a lot of the question is often how do we scale up her ability to create more content without becoming a massive studio. Drawing things takes time. Illustrating things takes time and budgets on a lot of AAA titles and kind of the studio pipeline format budgets are often 50, 60, 70 percent of the total production budget. So obviously like art is becoming increasingly more expensive and increasingly difficult to scale. So what if we could use AI in the asset pipeline? This was kind of the question we asked ourselves originally when we started our company and we've been kind of building tools to sort of help in this direction. So a quick quiz for the chat. So we're building AI tools. Which one of the following images was not actually drawn by a human? So I'll give 10 seconds or so. So these are three illustrations in the anime style of character portraits and it turns out the right one is actually drawn entirely by our AI. The left two are from popular Twitter artists and the key thing here is we can actually create images that are on par with what an illustrator would be able to draw. So we can basically in the amount of time, so the left two images would have probably taken a professional illustrator anywhere from 2 to 15 hours in order to draw and our tool can actually draw a character in sub two seconds and that's not all. We can not only can we draw a character but we can also we can draw hundreds of characters in the same amount of time that it would take to generate one character. So the possibilities are kind of kind of endless in that respect. So here's an example. We actually have one of our older models online that one of this model is called on waifulabs.com and what you see here is you can actually interact with one of our earlier models. You have the ability to pick any character and then customize this character using various steps through the AI through the online flow. So yes yes I am reading the chat we were the we were the ones behind. So how does this work? So I'll give a brief introduction to how did this technology works the the ability the way we are able to kind of create characters from scratch. The general idea is we're using a technology called GANs or generative adversarial networks. The way this works is we have a network a neural network called a generator and we have a second neural network called a discriminator. The generator's job is to learn how to draw art and the discriminator's job draw art and the discriminator's job is to learn how to tell good art from fake art. So we can take these two agents and we can actually construct a network in the following way. We take the generator and we take a corpus of real art that we want the generator to learn how to draw like. In this in this example we have a bunch of classical paintings so we actually want the generator to learn how to draw classical paintings. We can take these two images and feed them at random to the discriminator. So the discriminator's job is now to decide whether an image came from the real corpus or from the generator's learn to draw drawings. What happens is this is then fed we can then evaluate whether the discriminator was correct in his assessment and then we back propagate and update the weights for both the generator and the discriminator so that both of them now can learn from whether they made a mistake or not. This then gets propagated and this is how the cycle learns we run this millions upon millions of time in order to train both the generator to the discriminator. One small note but this isn't quite important is that both of the generator and the discriminator are computer programs so they are idempotent they will always give back the same result every time so we actually have to feed in some noise into the generator which we call the latent space but this noise allows the generator to create different images. So if both of them are able to learn if we're able to teach both the generator and the discriminator proper values we can actually show that at some point the generator can produce images that are indistinguishable from the real data set. So the generator can now create high quality images that almost that basically look like they come from the real distribution of data and then because we were feeding in a random noise in order to make the generator generate new images this actually comes in handy because we can actually now control the output of the generator from the the latent noise that we feed into it. So as an example we can take a train generator and generate the same character in multiple different expressions. We can take the same character and generate it in multiple different colors and we can take the same character and even transfer the style or completely change the illustration style for the same character and these are sorts of tasks that would take a trained artist hours at minimum in order to do but when you're a small art director you could you could do multiple different variations very quickly using using the power of the AI. So talk a little bit about data set. We train our network by basically crawling publicly available images off the internet. We're starting with the anime aesthetic mostly because it has the most amount of data available but there are about 10 million images available to train. One interesting thing to note about our data set is that the distribution doesn't actually follow traditional ML data sets. In particular the skew is actually very female oriented so girls outnumber boys in terms of anime illustrations about one to six and then darker skin tones people of color tend to be less than three percent of the total number of illustrations available on the internet. So we actually spend a lot of effort in order to correct this because obviously like these percentages don't represent the real world and representation is important especially for illustration. So what we've done is we've improved the generation of darker skin tones so our AI can actually draw at a higher frequency that you would actually see in quote the real world data set even though it doesn't represent actual population statistics as well as improving the generation of male characters. A fun fact like illustrators actually especially in Japan don't actually even like drawing male characters because female characters often give them more likes and more retweets so it's actually hard to find high quality male characters but using the power of AI you can actually draw things that normal illustrators wouldn't even want to draw. We have a number of other active areas of research for instance automated animation on the right side I have a character is fully generated and also fully animated using our workflow a number of live 2d spine workflow assistance tools and some super resolution-based techniques for animation processes. So obviously training the system is quite complex so I'm going briefly into how we train it. We've built our own small mini supercomputer to do this because the cloud is actually quite expensive so here you see us loading a 42u rack into our our office. We've built basically a DIY supercomputer here with a top like 100 gigabit ethernet top of rack router 200 plus cores 20 plus GPUs and a boatload of storage running on our system. A big question people often ask me is like what about cloud? Closest comparable machine on the cloud would be an AWS P3 16x large which on demand is about 24 an hour even if you use spot instances you maybe cut it in half to about ten dollars an hour. The key thing is training these models is actually quite expensive because it takes us about seven to ten days in order to train which means every individual model costs us somewhere between three to four thousand dollars so that obviously makes us very sad so that's why we've built a kind of in a scrappy startup way an entire cluster in our office. Yes it can definitely run Crysis Alex because these are all Titan RTX GPUs so our total running cost is about 60 cents an hour once everything is accounted for. So a brief overview of our architecture we have a custom language internally called NetGen which allows us to describe GAN architectures very quickly. It compiles into tensorflow low-level ops these tensorflow low-level ops get packaged into singularity containers we then schedule it onto our large cluster on slurm you can see the Crysis capable GPUs down there the eight Titan RTXs is one of our nodes which we run our workloads on then data gets piped out Prometheus Grafana standard and then as well as a tensor board for tracking loss loss functions. So we're taking all this technology and we're building internally the world's first AI illustrated game. Right now we're a very small team we got five people but we're looking to hire our sixth person so if any of you guys if the picture on the right resonates with any of you you probably want to give us a shout out Aqua dancing on top of like I don't know about 40 terabytes worth of flash. So we're hiring artists we're looking for a 2d animator and motion designer and we're also looking for a real-time VFX artist and also we are looking for an AI research intern for the coming winter and if this sounds good to you ping us at jobs at smallbrush.com I'll probably be in the breakout room later to answer any questions you guys might have but otherwise thank you so much